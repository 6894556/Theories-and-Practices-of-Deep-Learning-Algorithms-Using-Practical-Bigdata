{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. 영화","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOzBzAN4e2VDC6qbg43xl08"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["RNN으로 영화평 감성 분석하기 2022년 1월 10일 김이룸"],"metadata":{"id":"KEtM5mIPWKqy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9O66FVW6V_fW"},"outputs":[],"source":["# 1번 셀\n","\n","# 패키지 수입\n","import numpy as np\n","from keras.datasets import imdb # 이미 데이터 처리가 되어있음\n","from keras.preprocessing.sequence import pad_sequences\n","from time import time\n","\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from sklearn.metrics import confusion_matrix, f1_score # f1 점수는 혼동행렬과 같이 따라다닌다."]},{"cell_type":"code","source":["# 2번 셀\n","\n","# 하이퍼 파라미터 설정\n","# 사전 안에 단어 수 (사용 빈도수로 sorting)\n","# 가장 많이 사용되는 단어 : the\n","# 단어수(MY_WORDS) 많다고 무조건 좋은가? 답 : no. 파라매터가 34만개로 늘어났다. 많이 안쓰는 단어도 학습해서 별로 효과를 보지 못했다.\n","MY_WORDS = 5000       # 5000 -> 10000사전 안에 단어 수 : 임의로 선택한 단어가 아니다. 특정한 조건을 갖고 선택한 단어들이다. 가장 많이 사용된 단어들이다. (기억하시오) -> 설계도 상 10000으로 표가된 수.\n","# 영화평 통일된 길이 -> 길게 해봐라\n","MY_LEN = 80           # 80 -> 200영화평 길이 : 길이란 단어 수. 각각의 영화평마다 길이가 다르다. 입력의 크기가 다르면 학습을 못한다. 입력을 통일해야한다. 모든 단어를 80개에서 잘러? -> 튜닝해서 재미봤다.\n","MY_EMBED = 32          # 임배딩 차원, \n","MY_HIDDEN = 64         # LSTM의 차원 : 숫자 계산해서 오른쪽으로 몇개 넘겨주는지(여행자수 300개) \n","\n","MY_EPOCH = 10         # 반복 학습 수: 10 -> 100 : 튜닝해서 별로 재미 못봄\n","MY_BATCH = 200          # 한번에 처리하는 데이터 수 -> 학습속도와 관련이 있다./"],"metadata":{"id":"RM3J3B7VWDrF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["요점정리\n","\n","\n","1.  Token 처리\n","2.   항목 추가\n","\n"],"metadata":{"id":"OVTvj1oHcHN-"}},{"cell_type":"code","source":["# 3번 셀\n","\n","# 데이터 불러오기 : keras가 사분할 해놨다.\n","# num_words : 사전에 단어를 몇개 쓸거냐\n","(X_train, Y_train),(X_test, Y_test) = imdb.load_data(num_words=MY_WORDS) # 총 5만개 들어있음. 학습용 2.5만개, 평가용 2.5만개.\n","\n","# 샘플 출력 : 엑셀파일이 아니니 여기서 찎어야함\n","# IMDB의 영화평은 token 처리 되어 있음\n","# token : 1, 14, 12 각각의 숫자가 token. -> 영어단어 하나하나가 고유의 정수(토큰번호로) 대응되었다.\n","print('학습용 데이터 0번 영화평 : ', X_train[0]) # 영화평이라면서 숫자만 있네? : 답 : 영화평 맞다.\n","# 각각의 숫자는 무엇을 의미하는 걸까? 답 : 단어를 숫자로 치환한 것이다. 어떤 단어인지는 모른다. 어떤 단어인지 알아내기 위해서는 다음과 같은 과정을 거쳐야 한다.\n","print('학습용 데이터 0번 감성 : ',Y_train[0]) # 감성 : 0은 부정, 1은 긍정\n","\n","# 기계는 단어로 학습 못한다. 숫자로는 학습할 수 있다. -> 영화평을 token 처리하면 기계가 학습할 수 있다.\n","\n","# 데이터 모양 확인\n","print('학습용 입력 데이터 모양 : ',X_train.shape) # (25000,)은 1차원 데이터다. 0번 영화평 같은게 2만 5천개 있다.\n","# 영화평 데이터는 2차원 데이터가 아니다. 1차원 데이터이다. 중요한 컨셉이다. 판서를 참고하시오. -> 명확하게 이해가 안된다!? keras가 4분할 했는데 1차원인 데이터가 정확히 어느것인가? X_train이 영화평 데이터 인가? 맞다. X_train에 영화평, Y_train에 해당 영화평에 대한 감성분석 라벨 데이터가 있다.\n","# 하나의 데이터가 영화평 하나이다.\n","# 1차원인 이유 : 영화평 길이가 다름.\n","print('학습용 출력 데이터 모양 : ',Y_train.shape)\n","# print(Y_train) # 영화평이 이진수로 표현되어 있다.\n","\n","print('평가용 입력 데이터 모양 : ',X_test.shape)\n","print('평가용 출력 데이터 모양 : ',Y_test.shape) # imdb는 학습용 반, 평가용 반으로 나눠져 있다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5eLIRaeYgSH","executionInfo":{"status":"ok","timestamp":1641800710846,"user_tz":-540,"elapsed":5023,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"90a9789f-1d08-4ae8-888f-f70caf088b2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n","학습용 데이터 0번 영화평 :  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n","학습용 데이터 0번 감성 :  1\n","학습용 입력 데이터 모양 :  (25000,)\n","학습용 출력 데이터 모양 :  (25000,)\n","평가용 입력 데이터 모양 :  (25000,)\n","평가용 출력 데이터 모양 :  (25000,)\n"]}]},{"cell_type":"code","source":["# 4번 셀\n","\n","# 단어를 정수로 전환하기 \n","word_to_id = imdb.get_word_index() # token 처리를 위해 사용한 파이썬 사전\n","print('the의 token : ', word_to_id['the']) # the를 넣으면 the의 token이 나온다.\n","print('virus의 token : ', word_to_id['virus']) # 코비드 전에는 3310보다 큰 숫자였다.\n","\n","\n","# 정수를 단어로 전환하기 : : token 처리된 영화평을 인간이 이해할 수 있게 전환하기 -> imdb에서 제공하지 않는다. 직접 구현해야 하나 어렵지 않다/\n","# word_to_id에는 단어들의 정수가 사용빈도수 대로 다 있다\n","# 사전을 처음부터 끝까지 \n","id_to_word = {} #  python 딕셔너리\n","for  word, id in word_to_id.items():\n","  id_to_word[id] = word\n","\n","print('token 1의 단어 : ', id_to_word[1])\n","print('token 2의 단어 : ', id_to_word[2])\n","print('token 3310의 단어 : ', id_to_word[3310])"],"metadata":{"id":"madgOcSZZLnL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641800710847,"user_tz":-540,"elapsed":21,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"c07d2d6c-3c87-4619-cf5b-3e7f0498d22b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","1654784/1641221 [==============================] - 0s 0us/step\n","the의 token :  1\n","virus의 token :  3310\n","token 1의 단어 :  the\n","token 2의 단어 :  and\n","token 3310의 단어 :  virus\n"]}]},{"cell_type":"code","source":["# 5번 셀\n","\n","# 영화평 단어로 전환\n","# 영화평에 특수 숫자(0,1,2,) 3개 추가되어 있음 -> the가 1이 아니라 the가 4야. 특수숫자 3개(0,1,2)가 사전 앞에  더해져서. 3을 다 빼야 돼. -> 그건 어떻게 하느냐? \n","# 0 : 패딩 -> 자리가 비면, 비어있으면  0으로 채워버리는 것. \n","# 1 : 영화평 시작 -> 1은 영화평 시작이라는 의미다. 이런 의미의 특수숫자다.\n","# 2 : 삭제 단어 -> 영화평 0번에서 2를 찾을 수 있다. 2는 삭제된 단어를 의미한다.\n","def decode(review):\n","  output = []\n","\n","  for i in review:\n","    \n","    # word = id_to_word[i-3] # 대괄호다. 소괄호 아니다. -> 3을 빼는 거다. 1에서 3을 빼서 에러가 생긴다. -> 0,1,2에서만 3빼면 안된다.\n","    word = id_to_word.get(i-3, '???') # 제대로 나오려면 3을 뺴야하는데. 음수가 나오면 문제가 발생한다. 음수가 발생해서 생긴 문제는 get함수로 처리하면 된다.\n","    output.append(word)\n","  print(output)\n","# dictionary.get() : dict의 Key로 Value얻기 # https://wikidocs.net/16\n","# decode(X_train[0]) # brilliant -> pos # 말이 안된다. 이게 영화평이라며. 전환했는데 왜 말이 안되는가? 답 : 영화평에 특수 문자 3개 추가되어 있어서.-> 0,1,2\n","decode(X_train[333]) # junk acting, bad director -> neg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kD1t-i9fvan","executionInfo":{"status":"ok","timestamp":1641800710848,"user_tz":-540,"elapsed":15,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"533883b4-328d-4f75-bffb-4f20057fbfe1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['???', 'with', 'title', 'like', 'this', 'you', 'know', 'you', 'get', 'pretty', 'much', 'lot', 'of', 'junk', 'acting', 'bad', 'script', 'bad', 'director', 'bad', '???', 'bad', 'br', 'br', 'movie', 'make', 'lot', 'of', 'noise', 'that', 'really', 'not', 'music', 'and', 'lot', 'of', 'people', '???', 'movie', 'make', 'bad', 'racial', 'stereotype', 'why', 'come', 'every', 'movie', 'with', 'black', 'hero', 'have', 'drug', '???', 'why', 'come', 'hero', 'always', 'have', 'to', 'dance', 'to', 'be', 'success', 'why', 'come', 'famous', '???', 'always', 'have', 'to', 'be', 'in', 'dance', 'movie', 'why', 'come', 'letter', 's', \"can't\", 'be', 'in', 'title', 'br', 'br', 'hollywood', 'need', 'to', 'stop', 'dumb', 'down', 'audience', 'and', 'make', 'movie', 'that', 'have', 'people', 'with', 'brain', 'who', 'know', 'how', 'speak', 'proper', 'english', 'br', 'br', 'do', 'self', 'favor', 'and', 'not', 'go', 'see']\n"]}]},{"cell_type":"code","source":["# 6번 셀\n","\n","\n","# 영화평 길이가 천차만별 -> 길이가 다르면 RNN에 넣어줄 수가 없다. 길이를 통일시켜줘야 입력할 수 있다.\n","for i in range(10):\n","  print('영화평 : ', i, '길이 :', len(X_train[i]))\n","\n","# 영화평 길이 통일\n","# truncate v. 자르다\n","# truncating='pre' : 앞부분 자르기\n","# truncating='post' : 뒷부분 자르기\n","# padding='pre' : 앞부분에 패딩처리. 앞부분을 (CNN padding='same'처럼)0으로 채우는 것.\n","# padding='post' : 뒷부분에 패딩처리. 뒷부분을 0으로 채우는 것.\n","# maxlen : 통일할 영화평의 길이 지정\n","X_train = pad_sequences(X_train,\n","                        truncating='pre',\n","                        padding='pre',\n","                        maxlen=MY_LEN) # 긴거는 자르고, 짦은 거는 채우기\n","print()\n","print()\n","for i in range(10):\n","  print('영화평 : ', i, '길이 :', len(X_train[i])) # 영화평 길이가 통일되었음을 확인할 수 있다.\n","\n","X_test = pad_sequences(X_test,\n","                        truncating='pre',\n","                        padding='pre',\n","                        maxlen=MY_LEN) # 긴거는 자르고, 짦은 거는 채우기"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQXOYettiKz7","executionInfo":{"status":"ok","timestamp":1641800711996,"user_tz":-540,"elapsed":1157,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"16b97250-b01a-4709-b8ef-591f5901d6e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["영화평 :  0 길이 : 218\n","영화평 :  1 길이 : 189\n","영화평 :  2 길이 : 141\n","영화평 :  3 길이 : 550\n","영화평 :  4 길이 : 147\n","영화평 :  5 길이 : 43\n","영화평 :  6 길이 : 123\n","영화평 :  7 길이 : 562\n","영화평 :  8 길이 : 233\n","영화평 :  9 길이 : 130\n","\n","\n","영화평 :  0 길이 : 80\n","영화평 :  1 길이 : 80\n","영화평 :  2 길이 : 80\n","영화평 :  3 길이 : 80\n","영화평 :  4 길이 : 80\n","영화평 :  5 길이 : 80\n","영화평 :  6 길이 : 80\n","영화평 :  7 길이 : 80\n","영화평 :  8 길이 : 80\n","영화평 :  9 길이 : 80\n"]}]},{"cell_type":"code","source":["# 7번 셀 : # 패턴이 이제 보여야 한다. -> 순서는 다 똑같다.\n","\n","# 데이터 모양 확인 : 인공신경망에 넣기 위함. \n","print('학습용 입력 데이터 : ', X_train.shape) # 2차원 되었다. 왜? 길이 맞췄으니까.\n","print('학습용 출력 데이터 : ', Y_train.shape) # 그대로 1차원\n","print('평가용 입력 데이터 : ', X_test.shape) # 2차원 되었다. 왜? 길이 맞췄으니까.\n","print('평가용 출력 데이터 : ', Y_test.shape) # 그대로 1차원\n","\n","# 인공신경망 구현하기 전에 설계도를 참고하라. : 여행자RNN 모델과 영화평RNN모델을 비교하시오. 공통점, 차이점 각각은 무엇인가.\n","# -> 영화평모델도 many-to-one 모델이다\n","# 맨 오른쪽에서 출력 - 공통\n","# Dense와 sigmoid- 공통\n","# 영화평 sigmoid: 이중분류. 0과 1일 각각의 확률 -> 0에 가까우면 0이고, 1에 가까우면 1이다\n","# 여행자 수 모델과의 차이점 : embed가 있다, 없다.\n","# 영화평 모델 RNN 설계도는 80단이다.(단어80) 여행자 RNN모델은 4단이다.\n","# 영화평모델 - 5000에서 32로 뚝 떨어진다. \n","# 영화평 하나주고 감성 맞추고, 영화평 하나주고 감성 맞추고를 반복한다.\n","\n","# 영화평 설계도 중 주목할 숫자 : 10000, 2\n","# 단어 한개가 1만개, 우리 코드에서는 5000이 되었다. 갑자기 왜 불어났을까? 답: \n","# 토큰처리된 정수 1개가 어떻게 10000개로 늘어났는가? 답 : 원핫인코딩 -> 단어임베딩 강의자료를 참고하시오. (굉장히 중요) -> 2차원 임베딩\n","# 숫자가 1만개 들어간다. 왜 1만개가 들어가는가? 답: \n","# 단어임베딩 : 임베딩은 소수다. 토큰 한자리 은정수다. (key 열 숫자는 token을 의미)\n","# 토큰화하면 숫자 하난데 왜 숫자를 더 많이 써서 단어 표현을 하느냐? \n","# 원핫인코딩 : 정수를 이진수로 바꾸는 것. 자릿수는 정하기 나름. -> 단어임베딩의 유용성 강의자료\n","# 자연어 처리에서 왜 원핫인코딩을 하는가?\n","# 기계는 정수도 모른다. 이진수만 안다. 왜? 디지털 컴퓨터니까.\n","# 여러분이 안해도 케라스가 원핫인코딩 처리해서 집어넣는다. -> 1개에서 10000개를 케라스가 알아서 한다.\n","# 7자리 2자리 -> 임베딩이 자리를 덜 차지한다.\n","# 사전에서 사용된 단어 몇개? 답 : 5000개\n","# 그럼 원핫인코딩에 사용된 이진수는 몇개 ? 답 : 5000개. 5천개자리수. -> 메모리에 저장하기는 너무 비효율적이다. -> 소수 두개로해서 메모리 절약하자. -> compact하게 임베딩하자.\n","# 코드에서 임베딩 : 5천개에서 2개로 줄이기.\n","# 임베딩 층 안에도 학습하는 가중치가 있다. -> Google Word2Vec -> 임베딩웨이트메트릭스, 이 '행렬'이 학습된다.\n","# Google Word2Vec : pca와 내용이 굉장히 비슷하다.\n","# 예 : 5차원을 4차원으로 줄이고 싶어. (강의자료 예)\n","# 우리 코드에서는 5천 차원에서 32차원으로 -> 자료에서는 1만차원에서 32차원으로 줄이기.\n","# 어떻게 차원을 줄여주느냐 ? 답 : 행렬을 곱하면 된다.\n","\n","# 임베딩 입력은 -> 원핫 벡터(1-hot-encoding된 라벨)가 들어온다 -> 원핫인코딩된 수가 들어온다.-> 네번쨰가 1이라 4번째 행이 출력된다.\n","# 행렬의 4행이 가중치고, 4행의 값이 변한다.\n","# 1만개에서 32개로 특징을 추출해서 함축한다 -> 임베딩행렬이 학습해서. \n","# 자연어 처리에서 임베딩은 반드시 필요하다. "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkUGihEMlcbB","executionInfo":{"status":"ok","timestamp":1641800711997,"user_tz":-540,"elapsed":8,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"4fbb0df2-ed41-40c8-87f1-5fd455ff27dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["학습용 입력 데이터 :  (25000, 80)\n","학습용 출력 데이터 :  (25000,)\n","평가용 입력 데이터 :  (25000, 80)\n","평가용 출력 데이터 :  (25000,)\n"]}]},{"cell_type":"code","source":["# 8번 셀\n","\n","# RNN 구현\n","model = Sequential()\n","\n","# 임베딩 층 추가\n","# 옵션이 세개 붙는다.\n","# input_dims : \n","# input _length : 몇단이 있느냐\n","model.add(Embedding(input_dim=MY_WORDS,\n","                    output_dim=MY_EMBED,\n","                    input_length=MY_LEN))\n","\n","# LSTM 추가 : 내가 다음 단으로 몇개를 넘겨줄지만 지정해주면 된다.\n","# LSTM이 몇단으로 지정되는지 어떻게 알아요? 답 : 알아서 해준다.\n","model.add(LSTM(units=MY_HIDDEN))\n","\n","# 출력층 : 마지막으로 나가는 수 하나니깐 units=1\n","# 숫자하나의 확률 : sigmoid\n","model.add(Dense(units=1,\n","                activation='sigmoid'))\n","\n","# RNN 요약 : 총 16만개 파라미터 갖는 RNN이 만들어졌다. -> Total params: 184,897가 정상\n","# 16만은 5천 곱 32 : 임베딩 행렬의 크기\n","# (None, 80, 32)  : 80단 32 출력?\n","model.summary()"],"metadata":{"id":"sA5B2-TtnTQw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641800715064,"user_tz":-540,"elapsed":3072,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"fd53f0ee-6b62-4647-bdd5-12b79ed26b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 80, 32)            160000    \n","                                                                 \n"," lstm (LSTM)                 (None, 64)                24832     \n","                                                                 \n"," dense (Dense)               (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 184,897\n","Trainable params: 184,897\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# 9번 셀\n","\n","# RNN 학습\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['acc']) # metrics=['acc'] : 정확도도 보여달라고 하는 것.\n","\n","print('학습시작')\n","begin = time()\n","model.fit(X_train,\n","          Y_train,\n","          epochs=MY_EPOCH,\n","          batch_size=MY_BATCH,\n","          verbose=1)\n","\n","\n","end = time()\n","print('총 학습 시간 : ', end - begin)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpQ1Ok8Iwppo","executionInfo":{"status":"ok","timestamp":1641800751977,"user_tz":-540,"elapsed":36919,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"9193c8c9-835b-4816-ce30-6de331b80eb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["학습시작\n","Epoch 1/10\n","125/125 [==============================] - 8s 25ms/step - loss: 0.5239 - acc: 0.7205\n","Epoch 2/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.3318 - acc: 0.8590\n","Epoch 3/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.2904 - acc: 0.8791\n","Epoch 4/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.2642 - acc: 0.8919\n","Epoch 5/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.2469 - acc: 0.9001\n","Epoch 6/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.2259 - acc: 0.9099\n","Epoch 7/10\n","125/125 [==============================] - 3s 26ms/step - loss: 0.2014 - acc: 0.9217\n","Epoch 8/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.1776 - acc: 0.9331\n","Epoch 9/10\n","125/125 [==============================] - 3s 25ms/step - loss: 0.1564 - acc: 0.9404\n","Epoch 10/10\n","125/125 [==============================] - 3s 26ms/step - loss: 0.1358 - acc: 0.9523\n","총 학습 시간 :  36.7720160484314\n"]}]},{"cell_type":"code","source":["# 10번 셀\n","\n","# RNN 평가하기 : 코드가 항상 똑같다\n","score = model.evaluate(X_test,\n","               Y_test)\n","\n","print('최종 정확도 : ', score[1]) # 10번 학습해서 81점이면 나쁜건 아니다. \n","\n","# 80점대를 90점대로 끌어올릴려면? \n","# 답안1: my_words = 100000 -> 결과:  no. 효과없음\n","# 답안2 : epoch 올리기 -> 학습데이터는 같다. 같은 데이터를 몇번 반복했느냐의 차이이다.epoch 올리면 이 차이다.\n","# 이코딩은 당장 써먹을 수 있다. -> 한글처리를 하면 한글도 사용가능하다.\n","# 에러 역추적 시 전역적을 봐야함."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6fduhZWyfkS","executionInfo":{"status":"ok","timestamp":1641800762741,"user_tz":-540,"elapsed":10792,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"c1a7e59e-4a4f-46a0-ea3b-237c2a039860"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 7s 8ms/step - loss: 0.5757 - acc: 0.8161\n","최종 정확도 :  0.8161200284957886\n"]}]},{"cell_type":"code","source":["# 11번 셀\n","\n","# RNN으로 예측\n","\n","pred = model.predict(X_test)\n","\n","# 평가용 데이터 샘플 예측\n","# print('평가용 영화평 0번 : ', X_test[0] ) # 왜 0으로 꽉차있어? 답 : 앞을 0으로 패딩함. 자릿수맞추기 위해.\n","print('평가용 영화평 0번 : ', decode(X_test[0])) # 부정적 평가임을 확인한다. 그리고어덯게 예측했느지 본다.\n","print('정답 : ', Y_test[0]) \n","print('예측 : ', pred[0]) # 예측이 고퀄 예측인가: 답 : 네. 0에 가까울 수록 0(부정)이고, 1에 가까울 수록 1(긍정)이므로 고퀄리티 예측이다. 거의 맞춘거다. 이정도면 성공이다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRK_u72GzD8T","executionInfo":{"status":"ok","timestamp":1641800768353,"user_tz":-540,"elapsed":5617,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"82d5edfe-0854-4799-ba96-ebd628d93f97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['???', '???', '???', '???', '???', '???', '???', '???', '???', '???', '???', '???', '???', 'please', 'give', 'this', 'one', 'a', 'miss', 'br', 'br', '???', '???', 'and', 'the', 'rest', 'of', 'the', 'cast', '???', 'terrible', 'performances', 'the', 'show', 'is', 'flat', 'flat', 'flat', 'br', 'br', 'i', \"don't\", 'know', 'how', 'michael', '???', 'could', 'have', 'allowed', 'this', 'one', 'on', 'his', '???', 'he', 'almost', 'seemed', 'to', 'know', 'this', \"wasn't\", 'going', 'to', 'work', 'out', 'and', 'his', 'performance', 'was', 'quite', '???', 'so', 'all', 'you', '???', 'fans', 'give', 'this', 'a', 'miss']\n","평가용 영화평 0번 :  None\n","정답 :  0\n","예측 :  [0.5331229]\n"]}]},{"cell_type":"code","source":["# 12번 셀\n","\n","# 혼동행렬 : 혼동행렬 만들기 위해서는 예측값이 확률값이면 안된다. 부정 긍정으로 만들어줘야한다. 이진수로 만들어줘야한다. \n","# 확률 결과\n","print(pred)\n","\n","# 확률 결과를 를 이진수로 전환\n","# 확률값을 이진수로 바꾸는 코드는 이하와 같다.\n","pred = (pred > 0.5)\n","print(pred)\n","\n","# 혼동행렬 찍기\n","print()\n","print()\n","print(confusion_matrix(Y_test,\n","                       pred))\n","\n","# [[10243  2257]\n","#   [ 2340 10160]]\n","# 에서 10243은 TP\n","# 에서 -> 채우고 맞는지 이메일로 질문하자\n","# 에서\n","# 에서 10160 : 부정적인 평이 답인데 예측을 부정으로 해서 예측이 맞은 경우 \n","# 여기서 TN은 무슨 뜻인가 : 부정적인 평이 답인데 예측을 부정으로 해서 예측이 맞은 경우 \n","# False(사실) Negative(예측) : neg로 예측해서 틀린 경우 \n","\n","# F1 점수\n","# average : 설명이 긴 관계로 생략한다. \n","print(f1_score(Y_test,\n","               pred,\n","               average='micro')) # 성적표에 최종 정확도와 같다. "],"metadata":{"id":"VbRjhHBC2wWk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641801062189,"user_tz":-540,"elapsed":5,"user":{"displayName":"IRUM KIM","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12875629916024573944"}},"outputId":"8171330f-facd-42be-ac7b-29c3914c692e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ True]\n"," [ True]\n"," [False]\n"," ...\n"," [False]\n"," [False]\n"," [ True]]\n","[[ True]\n"," [ True]\n"," [False]\n"," ...\n"," [False]\n"," [False]\n"," [ True]]\n","\n","\n","[[10243  2257]\n"," [ 2340 10160]]\n","0.81612\n"]}]},{"cell_type":"code","source":["# 심장병 판별하기 우리 애저로 했다. -> 심장병 판별하기 문제 스킵한다.\n","# 나중에 시간 남으면 심장병 해주겠다. \n","\n","\n","# 한줄한줄 전부 다 꼼꼼하게 가면 진도가 밀린다.\n","# 아직 설명 안한 것 : adam, LSTM"],"metadata":{"id":"ep4qeeik87m5"},"execution_count":null,"outputs":[]}]}